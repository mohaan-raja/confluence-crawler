<h1>Title: Bulk Import Experiment</h1><h2 id="BulkImportExperiment-Existingapproach:"><strong>Existing approach:</strong></h2><p>We create each record serially from the CSV data.</p><p>Let's take a scenario of Custom Modules. </p><p style="margin-left: 30.0px;">CSV has 10,000 rows to be imported. There are 2 custom fields associations for custom module. Thus, <span style="letter-spacing: 0.0px;">for every custom module entity to be created, there will be 3 inserts. </span></p><p style="margin-left: 30.0px;">So for complete CSV import, there will be <strong>30,000 insert queries</strong>.</p><h2 id="BulkImportExperiment-NewApproach:"><strong>New Approach:</strong></h2><p>We will use the gem -<a href="https://github.com/zdennis/activerecord-import" class="external-link" rel="nofollow"> ActiveRecord-import</a>.</p><p>Gem requires an array of active records as input. It will create and execute a single insert query for all records in the array.</p><p>As compared to the existing scenario, we can complete the task with <strong>3 insert queries. </strong></p><p><strong>Why not used till now in the product? </strong></p><p style="margin-left: 30.0px;">Gem has some major limitations:</p><ol><li>The only database operation involved in the gem is to execute the insert query.</li><li>Insert happens only for the provided model i.e. Associations of the model are not imported.</li><li>After insert, it won't fetch and return the records.</li><li>Since it does not fetch the records that are inserted, it does not support running the callbacks.</li></ol><p style="margin-left: 30.0px;">Since models and entire implementation needs to be tweaked, this approach was dropped and we went on with the traditional way of import.</p><h2 id="BulkImportExperiment-Implementation:"><strong>Implementation:</strong></h2><p>Recently Custom Modules was introduced in Freshsales. Being a new entity with minimal implementation, we had the opportunity to test the gem again.</p><p><strong>Assumptions and Decisions that are taken keeping performance in mind:</strong></p><ol><li>We will use gem to create an insert query of 100 active records at a time.</li><li>The model class will not include any around callbacks. Since we will be triggering all callbacks manually, it will be hard to handle around callbacks.</li><li>Association decisions:<ol><li>Associations will be created up to one level. i.e. Association of Associations will not be handled.</li><li>As of now, there is no requirement to run callbacks on the association. We will handle it when required.</li><li>We are whitelisting the associations that need to be imported.</li><li>In case if any belongs-to association is present and need to be imported, we should consider it as parent entity and import it first. As of now, this is not yet implemented.</li><li>There is no delete operation performed. Even for associations, there should be only update-case. There should not be any recreate-case eg. contact_sales_accounts associations for contacts. If required, we will have to bulk delete. But as of now, no such use-case is present.</li></ol></li></ol><p><strong>The flow of execution:</strong></p><ul><li>From CSV, We will select a chunk of 100 rows.</li><li>We will build active record objects and validate. We will skip invalid records at this stage.</li><li>Execution inside a transaction block:<ul><li>Run before callbacks on the parent entity.</li><li>Create and execute bulk insert query for 100 active records.</li><li>Fetch imported records.</li><li>Create a list of association per model for eg. an array of CustomField1 records and execute Bulk Insert for each model.</li><li>Set previous_changes data from the stored changes to the fetched records. We will manually set it.</li><li>Run after callbacks</li></ul></li><li>Run after_commit callback on each record. This will run out of the transaction block.</li></ul><h2 id="BulkImportExperiment-Challenges:"><strong>Challenges: </strong></h2><p>Since the gem does not support anything other than Insert Query generation and Execution as per use case, we had to mock rails way manually.</p><ol><li>We have to store the changes of each active record and have to assign it back manually to a rails instance variable to give back previous_changes data.</li><li>We have to set the transaction state as create/update manually for each record so that implementation is not altered in the model.</li><li>We will have to trigger callbacks manually. <ol><li>The way rails work is, we can trigger callbacks in 2 ways:<ol><li>trigger before callbacks</li><li>trigger before and after callbacks together</li></ol></li><li>Since there is no way to trigger after_callbakcs alone, we have to add a check in before callbacks methods to skip it running the second time.</li></ol></li><li>Rails implementation is very big and abstracted at each level. As per current usecase we were able to mock rails way of handling features like callbacks etc. We will have to put extra efforts if any new use cases are found.</li></ol><h2 id="BulkImportExperiment-Results:"><strong>Results:</strong></h2><p>For a CSV of 10,000 rows for custom module:</p><div class="table-wrap"><table class="relative-table wrapped confluenceTable" style="width: 95.6134%;"><colgroup><col style="width: 12.1401%;"/><col style="width: 36.1089%;"/><col style="width: 51.751%;"/></colgroup><tbody><tr><th class="confluenceTh"><br/></th><th class="confluenceTh">Old Approach</th><th class="confluenceTh">New Approach</th></tr><tr><td class="confluenceTd"><strong>No. of insert queries</strong></td><td class="highlight-red confluenceTd" data-highlight-colour="red" title="Background colour : Red"><ul title=""><li>30,000 queries<ul><li>1000 for custom_module1 table</li><li>1000 for custom_field_table1</li><li>1000 for custom_field_table2</li></ul></li></ul></td><td class="highlight-green confluenceTd" data-highlight-colour="green" title="Background colour : Green"><ul title=""><li>300 queries(since the batch size is 100)<ul><li>100 for custom_module1 table</li><li>100 for custom_field_table1</li><li>100 for custom_field_table2</li></ul></li><li>Skipped extra <strong>29,700 queries.</strong></li></ul></td></tr><tr><td class="confluenceTd"><strong>Time Taken</strong><br/><strong>(Local Machine)</strong></td><td class="highlight-red confluenceTd" data-highlight-colour="red" title="Background colour : Red">Calculated in rails console:<br title=""/>396 sec + (variable time during import flow)<br title=""/>It might reach to 410 to 420 seconds easily.</td><td class="highlight-green confluenceTd" data-highlight-colour="green" title="Background colour : Green"><p title="">235 sec for import flow.<br/><strong>Import is close to 2X faster.</strong></p><p title="">As the number of associations increases, this will become much faster as compared to the old approach.</p></td></tr><tr><td colspan="1" class="confluenceTd"><strong>Time Taken</strong><br/><strong>(Vijay Rails Stack)</strong></td><td colspan="1" class="confluenceTd"><br/></td><td class="highlight-green confluenceTd" colspan="1" data-highlight-colour="green" title="Background colour : Green"><p>199 sec for Import flow.</p><p><strong>Per second - 50 records are created.</strong></p></td></tr><tr><td class="confluenceTd"><strong>Exception handling</strong></td><td class="highlight-green confluenceTd" data-highlight-colour="green" title="Background colour : Green"><ul title=""><li>Identify run time exceptions for each record and skip it alone</li><li>Resuming import can happen from the exact point it stopped.</li></ul></td><td class="highlight-red confluenceTd" data-highlight-colour="red" title="Background colour : Red"><ul title=""><li>We will have to skip a batch of 100 fully if any runtime exception occurs.</li><li>Resuming import will happen batch-wise only. i.e. even if 99 records were inserted properly since batch was not completed, we will rollback and on resuming again 99 records will be processed. </li></ul></td></tr></tbody></table></div>