<h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p><h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p><h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p><h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p><h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p><h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p><h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p><h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p><h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p><h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p><h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p><h1>Title: RCA Downtime 2017-03-27</h1><p><strong>Action that caused the incident:</strong> Replacement of will filter queries from DB to ES.<br/><strong>Use case the caused the problem:</strong> Cross module filter conditions (ContactsController#index)</p><p><strong>Alerts raised :-</strong><br/>  The following alerts were raised from 1:30 PM :<br/>    elasticsearch.cluster-1.cluster-1-data-8/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-6/opsworks.check_cpu_stats<br/>    elasticsearch.cluster-1.cluster-1-data-3/opsworks.check_cpu_stats</p><p>  Then the app went down at 1:40 PM for which we received the following alert :<br/>    URL Monitoring/<a href="http://Webapp.login.freshsales.io" class="external-link" rel="nofollow">Webapp.login.freshsales.io</a></p><p><strong>Actions taken :</strong><br/>1. The flag to send will filter queries to ES was disabled : <strong>$redis_report.set('IS_ES_FILTER_DISABLE', 1)</strong><br/>2. The above step would have solved the problem if the current requests finished execution, but the requests locked the passenger processes.<br/> This can be verified by running &quot;<strong>sudo passenger-status</strong>&quot; command on the app instance.<br/> If the response shows that there is an active session and the last used time is very high, it indicates that the passenger is held up by a request.<br/>3. In this case restart nginx : <strong>sudo service nginx restart</strong></p><p>Since the flag was disabled, after restart the issue did not resurface. But at this point we were yet to identify which scenario caused the issue.</p><p>4. We tried checking newrelic for slow transactions : <a href="https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions" class="external-link" rel="nofollow">https://rpm.newrelic.com/accounts/1147877/applications/12261525/transactions</a>.<br/> Everything seemed normal there. So we decided to check the logs.<br/>5. Since we had enabled the code change only for <a href="http://freshdesk.freshsales.io" class="external-link" rel="nofollow">freshdesk.freshsales.io</a> account, it was sufficient to check that account's logs.<br/>6. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; | grep freshdesk | grep Started</strong><br/> This gave the list of all requests from 8:00 AM to 8:10 AM UTC. We needed the execution time of the requests as well.<br/>7. Command : <strong>cat production.log | grep &quot;03-27 08:0&quot; -C 5 | grep freshdesk | grep 'Started\|Completed'</strong><br/> This was the last line of the output after which there were no requests till after we restarted nginx.<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Started GET &quot;/contacts?include=sales_account%2Cowner%2Ccreater%2Cupdater%2Csource%2Ccampaign&amp;page=1&amp;per_page=25&amp;segment_id=1000000557&amp;sort=lead_score&amp;sort_type=desc&amp;_=1490601761453&quot; for 157.48.3.45 at 2017-03-27 08:03:02 +0000<br/>8. Command : <strong>cat production.log | grep &quot;70f27bb5f8c0c0c3c8a3b4d4b5fe2790&quot;</strong><br/> It contained the time taken for the request. (There was no response code in the last line)<br/> [freshdesk] [70f27bb5f8c0c0c3c8a3b4d4b5fe2790] Completed in 932189.8ms <br/>9. Command : <strong>cat production.log-20170328 | grep &quot;Completed in&quot; | grep freshdesk</strong><br/> This command is to find all the requests to freshdesk account that took long time. It contained all the 12 requests that were blocking the passenger. All the requests were to ContactsController#index and all those calls contained cross module(deal) filter conditions.</p>